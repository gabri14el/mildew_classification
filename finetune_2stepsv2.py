import torch
from torch.utils.data import DataLoader
from torch.optim.lr_scheduler import CosineAnnealingLR
#import mlflow
from torchvision.datasets import ImageFolder

import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
import os
from torchvision.models import swin_transformer, resnet50, mobilenet_v3_large, efficientnet_b4, maxvit_t, convnext_tiny,vit_b_16, densenet121
import random

from PIL import ImageFilter

from torcheval.metrics import MulticlassF1Score, MulticlassAccuracy
import mlflow
import utils
import matplotlib.pyplot as plt
import json
import torch
import math
import warnings
from timm.data.mixup import Mixup
from timm.loss import LabelSmoothingCrossEntropy, SoftTargetCrossEntropy
from timm.utils import accuracy


num_epochs = 1*2

#dataset_path = '/mnt/c/Users/Gabriel/Downloads/castas-huge2-split'
#dataset_path = '/mnt/c/Users/Gabriel/Downloads/Castas-Dataset-V2-split'
#dataset_path = '/media/gabriel/7739-DDF5/Gabriel/Downloads/castas/Castas-Dataset-V2-newSplit/'
#dataset_path = '/home/gabriel/Downloads/castas-huge2-split/'
dataset_path = '/mnt/c/Users/Gabriel/Downloads/DatasetMildioESA/all'
batch_size = 16
input_size = 224
experiment_name = "all"
model_function = vit_b_16
head_name = "heads"
lr = 0.001
wd = 0.0005
comments = "Initial training"

#mixup
mixup = 0.8
cutmix = 1.0
cutmix_minmax = None
mixup_prob = 0.5
mixup_switch_prob = .5
mixup_mode = 'batch'

#label smoothing
smoothing = 0.1

#models = [resnet50, swin_transformer.swin_t, mobilenet_v3_large, efficientnet_b4, maxvit_t, convnext_tiny, vit_b_16, densenet121]
#tops = ['fc', 'head', 'classifier', 'classifier', 'classifier', 'classifier', 'heads']

models = [efficientnet_b4, maxvit_t, convnext_tiny, vit_b_16, densenet121]
tops = [ 'classifier', 'classifier', 'classifier', 'heads', 'classifier']


class GaussianBlur(object):
    """Gaussian blur augmentation in SimCLR https://arxiv.org/abs/2002.05709"""

    def __init__(self, sigma=[.1, 2.]):
        self.sigma = sigma

    def __call__(self, x):
        sigma = random.uniform(self.sigma[0], self.sigma[1])
        x = x.filter(ImageFilter.GaussianBlur(radius=sigma))
        return x
    
def compute_f1_score(predictions, labels):
    true_positives = torch.sum(predictions * labels)
    false_positives = torch.sum(predictions * (1 - labels))
    false_negatives = torch.sum((1 - predictions) * labels)
    
    precision = true_positives / (true_positives + false_positives + 1e-7)
    recall = true_positives / (true_positives + false_negatives + 1e-7)
    
    f1_score = 2 * (precision * recall) / (precision + recall + 1e-7)
    
    return f1_score

def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    # type: (Tensor, float, float, float, float) -> Tensor
    r"""Fills the input Tensor with values drawn from a truncated
    normal distribution. The values are effectively drawn from the
    normal distribution :math:`\mathcal{N}(\text{mean}, \text{std}^2)`
    with values outside :math:`[a, b]` redrawn until they are within
    the bounds. The method used for generating the random values works
    best when :math:`a \leq \text{mean} \leq b`.
    Args:
        tensor: an n-dimensional `torch.Tensor`
        mean: the mean of the normal distribution
        std: the standard deviation of the normal distribution
        a: the minimum cutoff value
        b: the maximum cutoff value
    Examples:
        >>> w = torch.empty(3, 5)
        >>> nn.init.trunc_normal_(w)
    """
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)


# Define your dataset and data augmentation transforms
normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                     std=[0.229, 0.224, 0.225])

train_transform = transforms.Compose([transforms.RandomResizedCrop(input_size, scale=(0.2, 1.)),
        transforms.RandomApply([
            transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)  # not strengthened
        ], p=0.8),
        transforms.RandomGrayscale(p=0.2),
        transforms.RandomApply([GaussianBlur([.1, 2.])], p=0.5),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        normalize
])

test_transform = transforms.Compose([
    transforms.Resize(input_size),
    transforms.ToTensor(),
    normalize
])

train_ds = ImageFolder(root=os.path.join(dataset_path, 'train'), transform=train_transform)
val_ds = ImageFolder(root=os.path.join(dataset_path, 'val'), transform=test_transform)
test_ds = ImageFolder(root=os.path.join(dataset_path, 'test'), transform=test_transform)

num_classes = len(train_ds.classes)

mixup_fn = None
mixup_active = mixup > 0 or cutmix > 0. or cutmix_minmax is not None
if mixup_active:
    print("Mixup is activated!")
    mixup_fn = Mixup(
        mixup_alpha=mixup, cutmix_alpha=cutmix, cutmix_minmax=cutmix_minmax,
        prob=mixup_prob, switch_prob=mixup_switch_prob, mode=mixup_mode,
        label_smoothing=smoothing, num_classes=num_classes)


for i, model_function in enumerate(models):
    new_head = nn.Sequential()
    head_name = tops[i]

    # Define your model
    #model = swin_transformer.swin_t(weights="IMAGENET1K_V1")
    #n_inputs = model.head.in_features
    #model = model_function(weights="IMAGENET1K_V1")
    model = model_function(weights="IMAGENET1K_V1")

    if isinstance(model.__dict__['_modules'][head_name], nn.Sequential):
        
        for m in model.__dict__['_modules'][head_name]:
            
            if isinstance(m, nn.Dropout):
                continue
            if not isinstance(m, nn.Linear):
                new_head.append(m)
            else:
                n_inputs = m.in_features
                break
    else: 
        n_inputs = model.__dict__['_modules'][head_name].in_features


    # Freeze model parameters
    for param in model.parameters():
        param.requires_grad = False

    #new_head.append(nn.Dropout(0.2))
    new_head.append(nn.Linear(n_inputs, 2048))
    new_head.append(nn.GELU())
    new_head.append(nn.LayerNorm(2048))
    #new_head.append(nn.Dropout(0.2))
    new_head.append(nn.Linear(2048, num_classes))
    #new_head.append(nn.Linear(n_inputs, num_classes))

    


    for m in new_head:
        if isinstance(m, nn.Linear):
                trunc_normal_(m.weight, std=.02)
                if isinstance(m, nn.Linear) and m.bias is not None:
                    nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)

    model.__dict__['_modules'][head_name] = new_head

    for param in model.__dict__['_modules'][head_name].parameters():
        param.requires_grad = True

    print(new_head)


    print(model)

    if mixup_fn is not None:
        # smoothing is handled with mixup label transform
        criterion = SoftTargetCrossEntropy()
    elif smoothing > 0.:
        criterion = LabelSmoothingCrossEntropy(smoothing=smoothing)
    else:
        criterion = torch.nn.CrossEntropyLoss()

    #eval criterion
    criterion2 = torch.nn.CrossEntropyLoss()

    print("criterion = %s" % str(criterion))

    # Define your optimizer and weight decay
    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)

    # Define your learning rate scheduler
    scheduler = CosineAnnealingLR(optimizer, T_max=10)

    # Define your data loaders
    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, pin_memory=True, drop_last=True)
    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, pin_memory=True)
    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, pin_memory=True)

    device = torch.device("cuda:1")
    model.to(device)
    metric = MulticlassF1Score(average="macro", num_classes=num_classes)
    metric2 = MulticlassAccuracy(average="macro", num_classes=num_classes)
    print(f"Starting training of {model_function.__name__}...")

    mlflow.set_experiment(experiment_name)
    run_name = model_function.__name__
    path_save =f'{model_function.__name__}_best.pth'
    mlflow.start_run(run_name=run_name)
    best_f1_score = 0.0
    # Training loop
    for epoch in range(int(num_epochs/2)):
        model.train()
        if epoch < 10:
            for param_group in optimizer.param_groups:
                param_group['lr'] = lr * (epoch + 1) / 10

        train_f1_score = 0.0
        train_loss = 0.0
        for images, labels in train_loader:
            #images, labels = images.to(device), torch.eye(num_classes)[labels].to(device)
            images, labels = images.to(device), labels.to(device)
            if mixup_fn is not None:
                images, labels = mixup_fn(images, labels)
            # Perform forward pass
            outputs = model(images)
            
            # Compute loss
            loss = criterion(outputs, labels)

            # Perform backward pass and optimization
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            train_loss += loss.item() * images.size(0)
            # Compute training metrics
            #predictions = torch.argmax(outputs, dim=1)
            #labels = torch.argmax(labels, dim=1)
            #metric.update(outputs, labels)
            # Update learning rate
            scheduler.step()
        
        #train_f1_score = metric.compute()
        train_loss /= len(train_loader.dataset)
        
        # Validation loop
        model.eval()
        val_f1_score = 0.0
        val_loss = 0.0
        metric.reset()
        metric2.reset()
        with torch.no_grad():
            for images, labels in val_loader:
                images, labels = images.to(device), labels.to(device)
                # Perform forward pass
                outputs = model(images)

                # Compute validation loss
                loss = criterion2(outputs, labels)
                val_loss += loss.item() * images.size(0)
                
                # Compute validation metrics
                predictions = torch.argmax(outputs, dim=1)
                #labels = torch.argmax(labels, dim=1)
                #print(predictions, labels)
                metric.update(predictions.cpu(), labels.cpu())
                metric2.update(predictions.cpu(), labels.cpu())
        # Compute average test F1 score
        val_f1_score = metric.compute()
        # Compute average validation loss
        val_loss /= len(val_loader.dataset)
        #Compute accuracy
        val_acc = metric2.compute()

        # Print the test F1 score
        
        log = f"Epoch: {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val F1 Score: {val_f1_score:.4f}, Val Acc: {val_acc:.4f}"
        if val_f1_score > best_f1_score:
            best_f1_score = val_f1_score
            torch.save(model.state_dict(), path_save)
            log+= ", Model saved"
        
        print(log)

    #model.load_state_dict(torch.load('model.pth'))

    model.load_state_dict(torch.load(path_save,  map_location=torch.device(device)), strict=True)

    
    # Freeze model parameters
    for param in model.parameters():
        param.requires_grad = True

    lr /= 10

    # Define your optimizer and weight decay
    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)

    # Define your learning rate scheduler
    scheduler = CosineAnnealingLR(optimizer, T_max=10)

    for epoch in range(int(num_epochs/2)):
        model.train()
        if epoch < 10:
            for param_group in optimizer.param_groups:
                param_group['lr'] = lr * (epoch + 1) / 10

        train_f1_score = 0.0
        train_loss = 0.0
        for images, labels in train_loader:
            #images, labels = images.to(device), torch.eye(num_classes)[labels].to(device)
            images, labels = images.to(device), labels.to(device)
            if mixup_fn is not None:
                images, labels = mixup_fn(images, labels)
            # Perform forward pass
            outputs = model(images)
            
            # Compute loss
            loss = criterion(outputs, labels)

            # Perform backward pass and optimization
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            train_loss += loss.item() * images.size(0)
            # Compute training metrics
            #predictions = torch.argmax(outputs, dim=1)
            #labels = torch.argmax(labels, dim=1)
            #metric.update(outputs, labels)
            # Update learning rate
            scheduler.step()
        
        #train_f1_score = metric.compute()
        train_loss /= len(train_loader.dataset)
        
        # Validation loop
        model.eval()
        val_f1_score = 0.0
        val_loss = 0.0
        metric.reset()
        metric2.reset()
        with torch.no_grad():
            for images, labels in val_loader:
                images, labels = images.to(device), labels.to(device)
                # Perform forward pass
                outputs = model(images)

                # Compute validation loss
                loss = criterion2(outputs, labels)
                val_loss += loss.item() * images.size(0)
                
                # Compute validation metrics
                predictions = torch.argmax(outputs, dim=1)
                #labels = torch.argmax(labels, dim=1)
                #print(predictions, labels)
                metric.update(predictions.cpu(), labels.cpu())
                metric2.update(predictions.cpu(), labels.cpu())
        # Compute average test F1 score
        val_f1_score = metric.compute()
        # Compute average validation loss
        val_loss /= len(val_loader.dataset)
        #Compute accuracy
        val_acc = metric2.compute()

        # Print the test F1 score
        
        log = f"Epoch: {int(num_epochs/2)+epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val F1 Score: {val_f1_score:.4f}, Val Acc: {val_acc:.4f}"
        if val_f1_score > best_f1_score:
            best_f1_score = val_f1_score
            torch.save(model.state_dict(), path_save)
            log+= ", Model saved"
        
        print(log)

    model.load_state_dict(torch.load(path_save,  map_location=torch.device(device)), strict=True)
    
    classes = list(test_ds.class_to_idx.keys())
    classes_alias = {'tinto cao': 'TC', 'tinta francisca': 'TF', 'alicante': 'AC', 'alveralhao': 'AV', 'arinto': 'AT', 'bastardo': 'BT', 'boal': 'BA', 'cabernet franc': 'CF', 'cabernet sauvignon': 'CS', 'carignon noir': 'CN', 'cercial': 'CC', 'chardonnay': 'CD', 'codega': 'CG', 'codega do larinho': 'CR', 'cornifesto': 'CT', 'donzelinho': 'DZ', 'donzelinho branco': 'DB', 'donzelinho tinto': 'DT', 'esgana cao': 'EC', 'fernao pires': 'FP', 'folgasao': 'FG', 'gamay': 'GM', 'gouveio': 'GV', 'malvasia corada': 'MC', 'malvasia fina': 'MF', 'malvasia preta': 'MP', 'malvasia rei': 'MR', 'merlot': 'ML', 'moscatel galego': 'MG', 'moscatel galego roxo': 'MX', 'mourisco tinto': 'MT', 'pinot blanc': 'PB', 'rabigato': 'RB', 'rufete': 'RF', 'samarrinho': 'SM', 'sauvignon blanc': 'SB', 'sousao': 'SS', 'tinta amarela': 'TA', 'tinta barroca': 'TB', 'tinta femea': 'TM', 'tinta roriz': 'TR', 'touriga francesa': 'TS', 'touriga nacional': 'TN', 'viosinho': 'VO'}
    report, report_dict = utils.confusion_matrix(test_loader, model, class_labels=classes,mode='pytorch', sns=True, normalize=False)

    mlflow.log_param("batch_size", batch_size)
    mlflow.log_param("dim", input_size)
    mlflow.log_param("dataset", dataset_path)
    mlflow.log_param("optimizer", 'AdamW')
    mlflow.log_param("lr",lr)
    mlflow.log_param("wd",wd)
    mlflow.log_artifact(path_save)
    mlflow.log_text(report, F"cm.txt")
    mlflow.log_text(json.dumps(report_dict), F"json_results.txt")
    mlflow.log_figure(plt.gcf(), 'cm.png')
    mlflow.log_param("loss", 'cross_entropy')
    mlflow.log_param("comments", comments or '')
    mlflow.log_metrics(report_dict['macro avg'])
    mlflow.log_param("input size", input_size)
    mlflow.log_param("model", model_function.__name__)
    mlflow.end_run()

    torch.cuda.empty_cache()    


